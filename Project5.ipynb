{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "Mjsp6j_mvHiL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ],
      "metadata": {
        "id": "UoDT_o2WGZc6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collect Data\n",
        "I also go ahead and make copies to play with feature selection"
      ],
      "metadata": {
        "id": "shZj3MNivMBx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9KUC2nSxFVc_"
      },
      "outputs": [],
      "source": [
        "filepath = \"/content/drive/MyDrive/Colab Notebooks/archive/Admission_Predict_Ver1.1.csv\"\n",
        "train_df = pd.read_csv(filepath)\n",
        "train_df_copy = train_df.copy()\n",
        "test_df = train_df.pop('Chance of Admit ')\n",
        "test_df_copy = train_df_copy.pop('Chance of Admit ')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df, test_df, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Selection\n",
        "Since I did not do much feature selection on past projects, I wanted to use the sklearn 'SelectKBest' function to select a few different feature sizes to test the cures of dimenstionality."
      ],
      "metadata": {
        "id": "N6wwcATrLu-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "three_feat = SelectKBest(chi2, k=3).fit_transform(train_df_copy.astype('int'), test_df_copy.astype('int'))\n",
        "X_train_3, X_test_3, y_train_3, y_test_3= train_test_split(three_feat, test_df_copy, test_size=0.2)\n",
        "feat3 = pd.DataFrame(three_feat)\n",
        "             \n",
        "five_feat = SelectKBest(chi2, k=5).fit_transform(train_df_copy.astype('int'), test_df_copy.astype('int'))\n",
        "X_train_5, X_test_5, y_train_5, y_test_5 = train_test_split(five_feat, test_df_copy, test_size=0.2)\n",
        "feat5 = pd.DataFrame(five_feat)"
      ],
      "metadata": {
        "id": "bxs4GkoPLvV6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## K-Fold\n",
        "To score all of the models I build, I wanted to use a single 5-fold cross validation function. "
      ],
      "metadata": {
        "id": "dKKEbFzyYSwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_val(data_x, data_y, model, num_folds=5):\n",
        "  kf = KFold(n_splits=num_folds)\n",
        "  avg = []\n",
        "  for train_index, test_index in kf.split(data_x):\n",
        "    X_train, X_test = data_x.iloc[train_index], data_x.iloc[test_index]\n",
        "    y_train, y_test = data_y.iloc[train_index], data_y.iloc[test_index]\n",
        "    model.fit(X_train, y_train)\n",
        "    avg += [model.score(X_test, y_test)]\n",
        "  return sum(avg)/len(avg)"
      ],
      "metadata": {
        "id": "l_SjULHkYS4s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "I feel the most basic model to start with is linear regression. This model assumes the data follows a linear trend throughout. Normalizing the data does not add any extra benefits for regression as it looks for functions to calculate an output rather than something else. Frin these models, it's clear that more features results in better accuracy, but that the keap between 5 and 8 features is likely the result of overfitting."
      ],
      "metadata": {
        "id": "q1iH9YqEKiZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lrm = LinearRegression()\n",
        "print(k_fold_cross_val(train_df,test_df,lrm))\n",
        "\n",
        "lrm = LinearRegression()\n",
        "print(k_fold_cross_val(feat3,test_df,lrm))\n",
        "\n",
        "lrm = LinearRegression()\n",
        "print(k_fold_cross_val(feat5,test_df,lrm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNM8zky6HevC",
        "outputId": "bfcd9f9f-0d14-45a5-cb1e-51c07d04d44e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7902348853116391\n",
            "0.7181017449412218\n",
            "0.7282410096350164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nearest Neighbors\n",
        "Nearest neighbors as a form of regression is somethign I have not worked with since coming to IU, and it allowed me to vaary my model in another dimension- the number of neighbors- for testing. Immediately, we can see that having too many parameters destroys the accuracy for a nearest neighbors model, which makes sense. Having so many ways to evaluate closeness means it's likely that unimportant features are weighted the saem as important ones. \n",
        "\n",
        "Another interesting finding is that 20 neighbors and 3 features begins to rival five features and any number of neighbors. This makes me wonder if 20 neighbors is beginning to cause unintended biases with 3 features and how these three variables are correlated."
      ],
      "metadata": {
        "id": "pmwnsYdgKkig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nnr = KNeighborsRegressor(n_neighbors=2)\n",
        "print(k_fold_cross_val(train_df,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=5)\n",
        "print(k_fold_cross_val(train_df,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=10)\n",
        "print(k_fold_cross_val(train_df,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=20)\n",
        "print(k_fold_cross_val(train_df,test_df,nnr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOchccgHKkny",
        "outputId": "28c7703f-debe-40a7-db5f-261c10dc1461"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.24904138398204623\n",
            "0.1945693987602522\n",
            "0.16405767160981316\n",
            "0.14268738614869997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnr = KNeighborsRegressor(n_neighbors=2)\n",
        "print(k_fold_cross_val(feat3,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=5)\n",
        "print(k_fold_cross_val(feat3,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=10)\n",
        "print(k_fold_cross_val(feat3,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=20)\n",
        "print(k_fold_cross_val(feat3,test_df,nnr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lhyznA5VCe8",
        "outputId": "ae29472d-5858-4e5e-97f1-fafcf0402e65"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4007445750865699\n",
            "0.4777529307487294\n",
            "0.5746809641270213\n",
            "0.6214390990332485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nnrnnr = KNeighborsRegressor(n_neighbors=2)\n",
        "print(k_fold_cross_val(feat5,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=5)\n",
        "print(k_fold_cross_val(feat5,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=10)\n",
        "print(k_fold_cross_val(feat5,test_df,nnr))\n",
        "\n",
        "nnr = KNeighborsRegressor(n_neighbors=20)\n",
        "print(k_fold_cross_val(feat5,test_df,nnr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTlNQjPoVK42",
        "outputId": "c14d750f-d619-4137-dbbe-52e7df7172a4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6339938038391458\n",
            "0.5646759345550985\n",
            "0.6277398144963\n",
            "0.6339938038391458\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree\n",
        "Last but not least, let's look at a decision tree. Here, I wanted to play around with setting a maximum tree depth to see how that impacted the various models. At a glance, a depth of ten was around what the models were maxing out was, but models with less dense trees performed better in all cases. Like linear regression, trees often can suffer from overfitting with features, but suprisingly both 3 and 5 features shared their accuracy quite well. "
      ],
      "metadata": {
        "id": "ETNOFoBWKkto"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor()\n",
        "print(k_fold_cross_val(train_df,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=3)\n",
        "print(k_fold_cross_val(train_df,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=5)\n",
        "print(k_fold_cross_val(train_df,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=10)\n",
        "print(k_fold_cross_val(train_df,test_df,dtr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cjy75QRKkyj",
        "outputId": "3af832d6-ff1d-47b3-cd97-55e23a44a136"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6733133286141734\n",
            "0.7104746451388882\n",
            "0.73902789755243\n",
            "0.6681541972267249\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor()\n",
        "print(k_fold_cross_val(feat3,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=3)\n",
        "print(k_fold_cross_val(feat3,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=5)\n",
        "print(k_fold_cross_val(feat3,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=10)\n",
        "print(k_fold_cross_val(feat3,test_df,dtr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5INlOOtuW8Nu",
        "outputId": "9f34c168-cee3-44a3-8244-6fdda99cdbb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.683499681752247\n",
            "0.6612554606991324\n",
            "0.6818847918740596\n",
            "0.683499681752247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor()\n",
        "print(k_fold_cross_val(feat5,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=3)\n",
        "print(k_fold_cross_val(feat5,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=5)\n",
        "print(k_fold_cross_val(feat5,test_df,dtr))\n",
        "\n",
        "dtr = DecisionTreeRegressor(max_depth=10)\n",
        "print(k_fold_cross_val(feat5,test_df,dtr))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0GhbNwgX5mo",
        "outputId": "6c5d7086-a5f7-42d4-f91d-c8932c1b3ffe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5966979152818437\n",
            "0.6718757196950599\n",
            "0.649684132379097\n",
            "0.6068445771442722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "Having tested a variety of different models for Linear Regression, Decision Tree Regression, and Nearest Neighbor Regression, I believe decision trees for regression to be the best. From what I know from classes and readings, the model can perform very well with little tweaking (especially when abstracted into a forest for ensemble learning.) Neural networks were certainly the most volatile due to the differences in how they're computer with the other models tested. There are tons of other regression models out there though,and I only played around in the space a small bit. I may add more models to the project, but they'll come after this bit unless I have more findings."
      ],
      "metadata": {
        "id": "WYg9gq0tgfd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To add for fun:\n",
        "- sklearn.ensemble.RandomForestRegressor\n",
        "- Neural networks like SVM?"
      ],
      "metadata": {
        "id": "sDoKAs7IyaTX"
      }
    }
  ]
}