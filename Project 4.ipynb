{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58952ad8",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "Since we're working with a few things I have only really discusses briefly or read in passing, this project should be a log of fun. I import many of the usual libraries I do, but for some of the newer libraries \n",
    "\n",
    "### nltk\n",
    "The natural language toolkit is a pretty amazing library that has all sorts of common NLP functions and various corpora. After doing some reading on what the common NLP libraries were, this one sprang up and I decided to give it a try. To install some corpora I had to run the following line:\n",
    "\n",
    "$$\\text{python -m textblob.download_corpora}$$\n",
    "\n",
    "### textblob\n",
    "This is another useful library for NLP but more specifically, sentiment analysis, noun-phrase extraction and tagging. I do not use it for much in the code, but a few of the examples I referenced used it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e126113b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\kanin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from string import punctuation\n",
    "from textblob import Word \n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5113f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set Some Constants\n",
    "seed_number = 35\n",
    "np.random.seed(seed_number)\n",
    "rev_col_name = 'review'\n",
    "sent_col_name = 'sentiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac32d93",
   "metadata": {},
   "source": [
    "## Import and Save Data Frame\n",
    "First, I need to read in all of the data files. This assumes that you're running this code in the working directory where you have your data stored, and that all of the data is still in it's default folders after being unpack. I add them all to a data frame to begin working on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64568c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kanin\\AppData\\Local\\Temp\\ipykernel_15596\\1475635634.py:11: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append([[txt, labels[l]]], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "folder_path = os.getcwd()\n",
    "\n",
    "labels = {'pos':1, 'neg':0}\n",
    "df = pd.DataFrame()\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(folder_path, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path,file), 'r', encoding=\"utf-8\") as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "df.columns = [rev_col_name, sent_col_name]\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8') #Saving a csv in case the other files break later"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8457d51",
   "metadata": {},
   "source": [
    "## Build Regression Model\n",
    "First, I wanted to just build just a normal Logistic Regression model without any cleaning. I did not thumb through the data, but I am assuming everything is relatively nice looking. \n",
    "NOTE: this takes quite a while. There are a lot of individual files. I thought of writng a script to combine them first, but then I realized I would have to wait the same amount if not more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26903d0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8889\n"
     ]
    }
   ],
   "source": [
    "#Get Data\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[rev_col_name],df[sent_col_name], shuffle=True, test_size=0.2, random_state=seed_number)\n",
    "\n",
    "#Make Model\n",
    "clf = Pipeline(steps =[('preprocessing', CountVectorizer()), ('classifier', LogisticRegression(dual=False,max_iter=2000))])\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test,y_test))\n",
    "p = clf.predict(x_test)\n",
    "t = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa957586",
   "metadata": {},
   "source": [
    "It looks like our accuracy is pretty good already which means just vectorizing our text was enough. Something tells me this shouldn't be the case. For now, I'll go ahead and K-Fold just to be sure this accuracy is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65f6ce2",
   "metadata": {},
   "source": [
    "## K-Fold Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "145c6428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8341000000000001\n"
     ]
    }
   ],
   "source": [
    "split_cnt = 5\n",
    "kf = KFold(n_splits=split_cnt)\n",
    "kf.get_n_splits(df[rev_col_name])\n",
    "data = []\n",
    "\n",
    "for train_index, test_index in kf.split(df[rev_col_name]):\n",
    "    x_train, x_test = df[rev_col_name][train_index], df[rev_col_name][test_index]\n",
    "    y_train, y_test = df[sent_col_name][train_index], df[sent_col_name][test_index]\n",
    "    clf = Pipeline(steps =[('preprocessing', CountVectorizer()), ('classifier', LogisticRegression(dual=False,max_iter=2000))])\n",
    "    clf.fit(x_train, y_train)\n",
    "    data += [clf.score(x_test,y_test)]\n",
    "print(sum(data)/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4824a",
   "metadata": {},
   "source": [
    "Well, it seems like the accuracy is reproducible. I'm still going to check the data and clean it up just in case we're experiecing any overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3e900b",
   "metadata": {},
   "source": [
    "## Clean Data Function\n",
    "I cannot take credit for everything hear. I did some reading on what sorts of things I could do to improve my accuracy. I'm not sure if we have any html tags or not, but all of the others are very applicable. I especially like removing all the most frequent words as they tend to add very little to your information (words like 'the', 'a', and things of the sort). Sadly, I was not able to get lemmatization to work due to some key errors. My guess is there is a name or something that is unrecognized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d0c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_reviews(frame):\n",
    "    #Remove HTML Tags\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(lambda words: re.sub('<.*?>','',words))\n",
    "    \n",
    "    #Word Tokenization\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(word_tokenize)\n",
    "    \n",
    "    #Convert to lower case\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(lambda words: [x.lower() for x in words])\n",
    "    \n",
    "    #Remove Punctuation\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(lambda words: [x for x in words if not x in punctuation])\n",
    "    \n",
    "    #Remove Numbers\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(lambda words: [x for x in words if not x.isdigit()])\n",
    "    \n",
    "    #Remove Frequent Words\n",
    "    temp = frame[rev_col_name].apply(lambda words: \" \".join(words))\n",
    "    freq = pd.Series(temp).value_counts()[:10] #removing 10 most common\n",
    "    frame[rev_col_name] = frame[rev_col_name].apply(lambda words: [x for x in words if x not in freq.keys()])\n",
    "    \n",
    "    #Lemmatization\n",
    "    #frame['review'] = frame['review'].apply(lambda words: \" \".join([Word(x).lemmatize() for x in words]))\n",
    "    frame['review'] = frame['review'].apply(lambda words: \" \".join(words))\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9407e50",
   "metadata": {},
   "source": [
    "## Cleaning Data \n",
    "Alright, time to clean up our data and re-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adbd3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_reviews(df)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df[rev_col_name],df[sent_col_name], shuffle=True, test_size=0.2, random_state=seed_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab500a4",
   "metadata": {},
   "source": [
    "## Build Regression Model (Pt. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cf42ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8884\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline(steps =[('preprocessing', CountVectorizer()), ('classifier', LogisticRegression(dual=False,max_iter=2000))])\n",
    "clf.fit(x_train, y_train)\n",
    "print(clf.score(x_test,y_test))\n",
    "p = clf.predict(x_test) \n",
    "t = y_test.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d47cd7c",
   "metadata": {},
   "source": [
    "Hmm, well that accuracy is basically the same for our purposes. Did all that cleaning mean nothing?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c36f213",
   "metadata": {},
   "source": [
    "## K-Fold Validation (Pt.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24b8948b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83492\n"
     ]
    }
   ],
   "source": [
    "review_kf = KFold(n_splits=split_cnt)\n",
    "kf.get_n_splits(df[rev_col_name])\n",
    "data = []\n",
    "\n",
    "for train_index, test_index in kf.split(df[rev_col_name]):\n",
    "    x_train, x_test = df[rev_col_name][train_index], df[rev_col_name][test_index]\n",
    "    y_train, y_test = df[sent_col_name][train_index], df[sent_col_name][test_index]\n",
    "    clf = Pipeline(steps =[('preprocessing', CountVectorizer()), ('classifier', LogisticRegression(dual=False,max_iter=2000))])\n",
    "    clf.fit(x_train, y_train)\n",
    "    data += [clf.score(x_test,y_test)]\n",
    "print(sum(data)/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e56216",
   "metadata": {},
   "source": [
    "Well, I guess the data was already cleaned enough and the count vectorization pulls a majority of the weight in creating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5d85c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Although I did not follow the book's code explicitly, I still followed along with what it was teaching me about Natural Language Processing and sentiment analysis. It was quite fun to actually dig my hands into a project like this especially because I have some research where I can turn around and use these skills. I opted to follow some other examples of this project so I could experience using some of these NLP libraries too (the one in the book was pretty cut and dry base python for the most part I felt). Overall, I'm surprised how much vectorizing the review does by itself. I almost wish I had worse data to see how much the cleaning could do. I feel like this project was a really good way to use logistic regression and NLP, and I look forward to doing the next one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7729bb0",
   "metadata": {},
   "source": [
    "## References\n",
    "- Textbook pages\n",
    "- https://textblob.readthedocs.io/en/dev/\n",
    "- https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "- https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
    "- https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/\n",
    "- https://kavita-ganesan.com/what-are-stop-words/#:~:text=Stop%20words%20are%20a%20set,on%20the%20important%20words%20instead.\n",
    "- https://monkeylearn.com/sentiment-analysis/\n",
    "- https://www.analyticsvidhya.com/blog/2015/10/6-practices-enhance-performance-text-classification-model/\n",
    "- https://www.nltk.org/\n",
    "- https://medium.com/@pyashpq56/sentiment-analysis-on-imdb-movie-review-d004f3e470bd\n",
    "- https://medium.com/hackerdawn/imdb-review-sentiment-analysis-using-logistic-regression-d7878ee01947\n",
    "- https://towardsdatascience.com/imdb-reviews-or-8143fe57c825"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
